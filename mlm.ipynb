{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import AdamW\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Inputs: tensor([[  101,  1996,  4937,  2938,  2006,  1996, 13523,  1012,   102],\n",
      "        [  101,  1996,  3899, 17554,  9928,  1012,   102,     0,     0],\n",
      "        [  101,  1996,  4743,  6369,  1037,  4086,  2299,  1012,   102],\n",
      "        [  101,  1996,  3869, 16849,  1999,  1996,  4951,  1012,   102],\n",
      "        [  101,  1996, 11190,  5598,  2058,  1996,  4231,  1012,   102]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Basudev\\genai-dsc-2024\\gaifndtion\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a small corpus of sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked loudly.\",\n",
    "    \"The bird sang a sweet song.\",\n",
    "    \"The fish swam in the tank.\",\n",
    "    \"The cow jumped over the moon.\"\n",
    "]\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs = tokenizer(sentences, return_tensors='pt', max_length=10, padding=True, truncation=True)\n",
    "print(\"Tokenized Inputs:\", inputs.input_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Masked Language Modeling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Inputs: tensor([[ 1595, 11477, 27706, 25821, 26699, 20856, 14835, 15736, 28743],\n",
      "        [  101, 12145, 26251, 15902, 11186,  9732, 28964,   103, 20150],\n",
      "        [  103, 26604, 18509, 12795, 27394,   103,  2299, 10067, 21141],\n",
      "        [  103, 22141,  3869, 21629,  1999, 12355, 29966, 26279, 26430],\n",
      "        [29299, 26198, 23535,  1624, 11300, 28357,   103, 23447, 22387]])\n",
      "Labels: tensor([[  101,  1996,  4937,  2938,  2006,  1996, 13523,  1012,   102],\n",
      "        [ -100,  1996,  3899, 17554,  9928,  1012,   102,     0,     0],\n",
      "        [  101,  1996,  4743,  6369,  1037,  4086,  -100,  1012,   102],\n",
      "        [  101,  1996,  -100, 16849,  -100,  1996,  4951,  1012,   102],\n",
      "        [  101,  1996, 11190,  5598,  2058,  1996,  4231,  1012,   102]])\n"
     ]
    }
   ],
   "source": [
    "def mask_tokens(inputs, tokenizer):\n",
    "    labels = inputs.clone()\n",
    "    # Replace 15% of tokens with [MASK]\n",
    "    mask_arr = torch.full(labels.shape, 0.15) < torch.rand(labels.shape)\n",
    "    labels[~mask_arr] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # Replace 80% of the masked tokens with [MASK]\n",
    "    inputs[mask_arr] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # Replace 10% of the masked tokens with a random word\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    random_replace = torch.full(labels.shape, 0.10) < torch.rand(labels.shape)\n",
    "    inputs[mask_arr & random_replace] = random_words[mask_arr & random_replace]\n",
    "\n",
    "    # Leave 10% of the masked tokens unchanged\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "masked_inputs, labels = mask_tokens(inputs.input_ids, tokenizer)\n",
    "print(\"Masked Inputs:\", masked_inputs)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pre-trained BERT Model and Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\Admin\\Desktop\\Basudev\\genai-dsc-2024\\gaifndtion\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 9.458168983459473\n",
      "Epoch 2, Loss: 8.453481674194336\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model for Masked Language Modeling\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Fine-tune the model\n",
    "model.train()\n",
    "epochs = 2  # For demonstration, use 2 epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(masked_inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Model with Masked Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The barked loudly.\n",
      "Masked Sentence: [CLS] the [MASK] loudly . [SEP]\n",
      "Predicted Word: door\n"
     ]
    }
   ],
   "source": [
    "# Select a sentence to test\n",
    "test_sentence = \"The barked loudly.\"\n",
    "inputs = tokenizer(test_sentence, return_tensors='pt')\n",
    "masked_input = inputs.input_ids.clone()\n",
    "\n",
    "# Mask a word in the sentence\n",
    "masked_input[0, 2] = tokenizer.mask_token_id  # Mask \"barked\"\n",
    "\n",
    "# Predict the masked word\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(masked_input)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# Get the predicted word\n",
    "predicted_index = torch.argmax(predictions[0, 2]).item()\n",
    "predicted_word = tokenizer.decode([predicted_index])\n",
    "\n",
    "print(f\"Original Sentence: {test_sentence}\")\n",
    "print(f\"Masked Sentence: {' '.join(tokenizer.convert_ids_to_tokens(masked_input[0]))}\")\n",
    "print(f\"Predicted Word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaifndtion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
